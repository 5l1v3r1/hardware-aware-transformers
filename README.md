# HAT: Hardware Aware Transformers for Efficient Natural Language Processing

under development; paper and code will be available soon. 

```bash
@inproceedings{
  wang2020hat,
  title={HAT: Hardware-Aware Transformers for Efficient Natural Language Processing},
  author={Wang, Hanrui and Wu, Zhanghao and Liu, Zhijian and Cai, Han and Zhu, Ligeng and Gan, Chuang and Han, Song},
  booktitle={Annual Conference of the Association for Computational Linguistics (ACL)},
  year={2020},
}
```

We present HAT framework to efficiently search efficient transformer architectures that are **specialized** for different hardware.
